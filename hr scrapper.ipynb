{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4dcf675f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def haryana_news():\n",
    "    import requests\n",
    "    import queue\n",
    "    # import telegram\n",
    "    from bs4 import BeautifulSoup\n",
    "    # import telegram.ext\n",
    "    # from telegram.ext import Updater, CommandHandler\n",
    "    import requests\n",
    "    from bs4 import BeautifulSoup\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    import html5lib\n",
    "    import lxml\n",
    "    import xlsxwriter\n",
    "    from datetime import date\n",
    "    import googletrans\n",
    "    from googletrans import Translator\n",
    "    import smtplib, ssl\n",
    "    from smtplib import SMTP_SSL\n",
    "    from email.mime.multipart import MIMEMultipart\n",
    "    from email.mime.text import MIMEText\n",
    "    from email.mime.base import MIMEBase\n",
    "    from email import encoders\n",
    "    from win32com import client\n",
    "    from datetime import datetime\n",
    "    import schedule\n",
    "    import time\n",
    "    from datetime import date\n",
    "    from datetime import timedelta\n",
    "    import textwrap\n",
    "    import psycopg2\n",
    "    import sqlalchemy\n",
    "    from sqlalchemy import create_engine, MetaData, Table, Column, Integer, String\n",
    "    import selenium\n",
    "    from selenium import webdriver\n",
    "    from selenium.webdriver.chrome.options import Options\n",
    "    from webdriver_manager.chrome import ChromeDriverManager\n",
    "    from selenium.webdriver.common.keys import Keys\n",
    "    from selenium.webdriver.common.by import By\n",
    "    from selenium.webdriver.support.ui import WebDriverWait\n",
    "    from selenium.common.exceptions import ElementClickInterceptedException\n",
    "    from selenium.webdriver.chrome.options import Options\n",
    "    #import pyshorteners\n",
    "    import tempfile\n",
    "\n",
    "    list_title_hr=[]\n",
    "    list_link_hr=[]\n",
    "    list_source_hr=[]\n",
    "    list_time_hr = []\n",
    "\n",
    "    date1=datetime.now()\n",
    "    date1 = date1.strftime(\"%d-%m-%Y %H-%M-%S\")\n",
    "\n",
    "    #Tribune HR\n",
    "\n",
    "    # url6 = \"https://www.tribuneindia.com/news/state/haryana\"\n",
    "    # headers = {\n",
    "    #     \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\"\n",
    "    # }\n",
    "\n",
    "    # # try:\n",
    "    #     # Send a GET request to the website\n",
    "    # r6 = requests.get(url6,headers = headers)\n",
    "\n",
    "    # # Create a BeautifulSoup object to parse the HTML content\n",
    "    # soup6 = BeautifulSoup(r6.content, \"html.parser\")\n",
    "\n",
    "    # # Find all the news articles card-top-align instead of ts-news-content\n",
    "    # news_articles = soup6.find_all(\"div\", class_=\"card-top-align\")\n",
    "\n",
    "\n",
    "    # # Iterate over the news articles and extract the headings and links\n",
    "    # for article in news_articles:\n",
    "    #     title = article.text.strip()\n",
    "    #     link = article.a[\"href\"]\n",
    "    #     list_title_hr.append(title.strip())\n",
    "    #     list_source_hr.append(\"TI_HR\")\n",
    "    #     list_link_hr.append('https://www.tribuneindia.com'+link)\n",
    "    #     list_time_hr.append(date1)\n",
    "\n",
    "    # # except Exception as e:\n",
    "    # #     print(\"Tribunehr\", str(e))\n",
    "\n",
    "    url7 = \"https://www.hindustantimes.com/cities/chandigarh-news\"\n",
    "    headers = {\n",
    "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\"\n",
    "    }\n",
    "    # list_title_hr1=[]\n",
    "    # list_link_hr1=[]\n",
    "    # list_source_hr1=[]\n",
    "    # list_time_hr1 = []\n",
    "\n",
    "    try:\n",
    "        # Send a GET request to the website\n",
    "        r7 = requests.get(url7, headers = headers)\n",
    "\n",
    "        # Create a BeautifulSoup object to parse the HTML content\n",
    "        soup7 = BeautifulSoup(r7.content, \"html.parser\")\n",
    "\n",
    "        # Find all the news articles\n",
    "        news_articles = soup7.find_all(\"h3\", class_=\"hdg3\")\n",
    "\n",
    "        # Iterate over the news articles and extract the headings and links\n",
    "        for article in news_articles:\n",
    "            title = article.text.strip()\n",
    "            link = article.find(\"a\")[\"href\"]\n",
    "            list_title_hr.append(title.strip())\n",
    "            list_link_hr.append('https://www.hindustantimes.com'+link)\n",
    "            list_source_hr.append(\"HT_HR&PB\")\n",
    "            list_time_hr.append(date1)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"HThrpb\", str(e))\n",
    "\n",
    "    # len(list_title_hr1)\n",
    "    # list_title_hr1\n",
    "\n",
    "    # list_link_hr1\n",
    "\n",
    "    url8 = \"https://haryana.punjabkesari.in/\"\n",
    "    # headers = {\n",
    "    #     \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\"\n",
    "    # }\n",
    "\n",
    "\n",
    "    try:\n",
    "        # Send a GET request to the website\n",
    "        r8 = requests.get(url8)\n",
    "\n",
    "        # Create a BeautifulSoup object to parse the HTML content\n",
    "        soup8 = BeautifulSoup(r8.content, \"html.parser\")\n",
    "\n",
    "        # Find the main container containing all the news articles\n",
    "        news_articles = soup8.find(\"ul\", class_=\"pargrap-n pr-4\").find_all(\"li\")\n",
    "\n",
    "\n",
    "        # Iterate over the news articles and extract the headings and links\n",
    "        for article in news_articles:\n",
    "            title = article.find(\"h3\").text.strip()\n",
    "            link = article.find(\"a\")[\"href\"]\n",
    "            list_title_hr.append(title.strip())\n",
    "            list_link_hr.append(link)\n",
    "            list_source_hr.append(\"PK_HR\")\n",
    "            list_time_hr.append(date1)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"PK\", str(e))\n",
    "\n",
    "    # len(list_title_hr)\n",
    "\n",
    "    url9 = \"https://www.haribhoomi.com/local/haryana\"\n",
    "        # headers = {\n",
    "        #     \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\"\n",
    "        # }\n",
    "\n",
    "    for i in range(1,4):\n",
    "        try:\n",
    "            # Send a GET request to the website\n",
    "            r9 = requests.get('https://www.haribhoomi.com/local/haryana/'+str(i)+'')\n",
    "\n",
    "            # Create a BeautifulSoup object to parse the HTML content\n",
    "            soup9 = BeautifulSoup(r9.content, \"html.parser\")\n",
    "\n",
    "            # Find the main container containing all the news articles\n",
    "            news_articles = soup9.find_all(\"div\", class_=\"list_content\")\n",
    "\n",
    "\n",
    "            # Iterate over the news articles and extract the headings and links\n",
    "            for article in news_articles:\n",
    "                title = article.text.strip()\n",
    "                link = article.find(\"a\")[\"href\"]\n",
    "                list_title_hr.append(title.strip())\n",
    "                list_link_hr.append('https://www.haribhoomi.com/'+link)\n",
    "                list_source_hr.append(\"hb_HR\")\n",
    "                list_time_hr.append(date1)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(\"HariBhoomi HR\", str(e))\n",
    "\n",
    "    # len(list_title_hr)\n",
    "\n",
    "    Zeenewsurl9 = [\"rewari\",\"chandigarh\",\"sonipat\",\"panipat\",\"bhiwani\",\"karnal\",\n",
    "                    \"kurukshetra\",\"rohtak\",\"jhajjar\",\"faridabad\",\"yamuna-nagar\",\"narnaul\",\"fatehabad\",\"mahendragarh\",\"sirsa\",\"palwal\"]\n",
    "    headers = {\n",
    "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\"\n",
    "    }\n",
    "\n",
    "    for i in Zeenewsurl9:\n",
    "        try:\n",
    "            # Send a GET request to the website\n",
    "            r9 = requests.get('https://zeenews.india.com/hindi/india/delhi-ncr-haryana/'+str(i)+'', headers=headers)\n",
    "\n",
    "            # Create a BeautifulSoup object to parse the HTML content\n",
    "            soup9 = BeautifulSoup(r9.content, \"html.parser\")\n",
    "\n",
    "            # Find the main container containing all the news articles\n",
    "            news_articles = soup9.find_all(\"div\", class_=\"news_title\")\n",
    "\n",
    "\n",
    "            # Iterate over the news articles and extract the headings and links\n",
    "            for article in news_articles:\n",
    "                title = article.text.strip()\n",
    "                link = article.find(\"a\")[\"href\"]\n",
    "                list_title_hr.append(title.strip())\n",
    "                list_link_hr.append('https://zeenews.india.com'+link)\n",
    "                list_source_hr.append(\"ZN_HR\")\n",
    "                list_time_hr.append(date1)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(\"ZNhr\", str(e))\n",
    "\n",
    "    # len(list_title_hr)\n",
    "\n",
    "    url10 = \"https://www.indianewsharyana.com/haryana/\"\n",
    "\n",
    "\n",
    "    try:\n",
    "        # Send a GET request to the website\n",
    "        r10 = requests.get(url10)\n",
    "\n",
    "        # Create a BeautifulSoup object to parse the HTML content\n",
    "        soup10 = BeautifulSoup(r10.content, \"html.parser\")\n",
    "\n",
    "        # Find the main container containing all the news articles\n",
    "        news_articles = soup10.find_all(\"h3\", class_=\"entry-title td-module-title\")\n",
    "\n",
    "\n",
    "        # Iterate over the news articles and extract the headings and links\n",
    "        for article in news_articles:\n",
    "            title = article.text.strip()\n",
    "            link = article.find(\"a\")[\"href\"]\n",
    "            list_title_hr.append(title.strip())\n",
    "            list_link_hr.append(link)\n",
    "            list_source_hr.append(\"INH_HR\")\n",
    "            list_time_hr.append(date1)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"INH\", str(e))\n",
    "\n",
    "    url10 = \"https://www.bhaskar.com/local/haryana/\"\n",
    "\n",
    "    try:\n",
    "        # Send a GET request to the website\n",
    "        r10 = requests.get(url10)\n",
    "\n",
    "        # Create a BeautifulSoup object to parse the HTML content\n",
    "        soup10 = BeautifulSoup(r10.content, \"html.parser\")\n",
    "\n",
    "        # Find the main container containing all the news articles\n",
    "        news_articles = soup10.find_all(\"li\", class_=\"c7ff6507 db9a2680\")\n",
    "\n",
    "\n",
    "        # Iterate over the news articles and extract the headings and links\n",
    "        for article in news_articles:\n",
    "            title = article.text.strip()\n",
    "            link = article.find(\"a\")[\"href\"]\n",
    "            list_title_hr.append(title.strip())\n",
    "            list_link_hr.append(\"https://www.bhaskar.com/\"+link)\n",
    "            list_source_hr.append(\"BH_HR\")\n",
    "            list_time_hr.append(date1)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"BH\", str(e))\n",
    "\n",
    "    # len(list_link_hr1)\n",
    "\n",
    "    # list_title_hr1\n",
    "\n",
    "    url10 = \"https://www.jagran.com/haryana\"\n",
    "\n",
    "\n",
    "    try:\n",
    "        # Send a GET request to the website\n",
    "        r10 = requests.get(url10)\n",
    "\n",
    "        # Create a BeautifulSoup object to parse the HTML content\n",
    "        soup10 = BeautifulSoup(r10.content, \"html.parser\")\n",
    "\n",
    "        # Find the main container containing all the news articles\n",
    "        news_articles = soup10.find_all(\"li\", class_=\"Editorial_CardStory__AOJMS CardStory\")\n",
    "\n",
    "\n",
    "        # Iterate over the news articles and extract the headings and links\n",
    "        for article in news_articles:\n",
    "            title = article.text.strip()\n",
    "            link = article.find(\"a\")[\"href\"]\n",
    "            list_title_hr.append(title.strip())\n",
    "            list_link_hr.append(\"https://www.jagran.com/\"+link)\n",
    "            list_source_hr.append(\"JG_HR\")\n",
    "            list_time_hr.append(date1)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"JG\", str(e))\n",
    "\n",
    "    # len(list_title_hr1)\n",
    "\n",
    "    # list_title_hr1\n",
    "\n",
    "    url9 = \"https://www.jagran.com/haryana\"\n",
    "\n",
    "    ll=['rohtak','hisar','panipat','yamunanagar','panchkula']\n",
    "\n",
    "    for i in ll:\n",
    "        try:\n",
    "            # Send a GET request to the website\n",
    "            r9 = requests.get('https://www.jagran.com/haryana/'+str(i)+'')\n",
    "\n",
    "            # Create a BeautifulSoup object to parse the HTML content\n",
    "            soup9 = BeautifulSoup(r9.content, \"html.parser\")\n",
    "\n",
    "            # Find the main container containing all the news articles\n",
    "            news_articles = soup9.find_all(\"li\", class_=\"ListingSide_CardStory__weOJf CardStory\")\n",
    "\n",
    "\n",
    "            # Iterate over the news articles and extract the headings and links\n",
    "            for article in news_articles:\n",
    "                title = article.text.strip()\n",
    "                link = article.find(\"a\")[\"href\"]\n",
    "                list_title_hr.append(title.strip())\n",
    "                list_link_hr.append(link)\n",
    "                list_source_hr.append(\"JG_HR\")\n",
    "                list_time_hr.append(date1)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(\"JG HR\", str(e))\n",
    "\n",
    "    # len(list_title_hr1)\n",
    "\n",
    "    # list_title_hr1\n",
    "\n",
    "    url10 = \"https://www.aajtak.in/india/haryana\"\n",
    "\n",
    "\n",
    "    try:\n",
    "        # Send a GET request to the website\n",
    "        r10 = requests.get(url10)\n",
    "\n",
    "        # Create a BeautifulSoup object to parse the HTML content\n",
    "        soup10 = BeautifulSoup(r10.content, \"html.parser\")\n",
    "\n",
    "        # Find the main container containing all the news articles\n",
    "        news_articles = soup10.find_all(\"div\", class_=\"widget-listing-content-section\")\n",
    "\n",
    "\n",
    "        # Iterate over the news articles and extract the headings and links\n",
    "        for article in news_articles:\n",
    "            title = article.text.strip()\n",
    "            link = article.find(\"a\")[\"href\"]\n",
    "            list_title_hr.append(title.strip())\n",
    "            list_link_hr.append(link)\n",
    "            list_source_hr.append(\"AAJ_HR\")\n",
    "            list_time_hr.append(date1)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"AAJ\", str(e))\n",
    "\n",
    "    # list_title_hr1\n",
    "\n",
    "\n",
    "    url10 = \"https://haryana.indianews.in/#\"\n",
    "\n",
    "\n",
    "    try:\n",
    "        # Send a GET request to the website\n",
    "        r10 = requests.get(url10)\n",
    "\n",
    "        # Create a BeautifulSoup object to parse the HTML content\n",
    "        soup10 = BeautifulSoup(r10.content, \"html.parser\")\n",
    "\n",
    "        # Find the main container containing all the news articles\n",
    "        news_articles = soup10.find_all(\"h3\", class_=\"entry-title td-module-title\")\n",
    "\n",
    "\n",
    "        # Iterate over the news articles and extract the headings and links\n",
    "        for article in news_articles:\n",
    "            title = article.text.strip()\n",
    "            link = article.find(\"a\")[\"href\"]\n",
    "            list_title_hr.append(title.strip())\n",
    "            list_link_hr.append(link)\n",
    "            list_source_hr.append(\"INDnews_HR\")\n",
    "            list_time_hr.append(date1)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"INDnews\", str(e))\n",
    "\n",
    "    # list_title_hr1\n",
    "\n",
    "    import requests\n",
    "    from bs4 import BeautifulSoup\n",
    "\n",
    "    url10 = \"https://www.indiatv.in/haryana\"\n",
    "\n",
    "\n",
    "    try:\n",
    "        # Send a GET request to the website\n",
    "        r10 = requests.get(url10)\n",
    "\n",
    "        # Create a BeautifulSoup object to parse the HTML content\n",
    "        soup10 = BeautifulSoup(r10.content, \"html.parser\")\n",
    "\n",
    "        # Find the main container containing all the news articles\n",
    "        news_articles = soup10.find_all(\"ul\", class_=\"list\")\n",
    "\n",
    "        c = 0\n",
    "        # Iterate over the news articles and extract the headings and links\n",
    "        for article in news_articles:\n",
    "            # Find all the li elements inside the ul\n",
    "            li_elements = article.find_all(\"li\")\n",
    "\n",
    "            for li in li_elements:\n",
    "                # Extract title and link from each li element\n",
    "                title = li.text.strip()\n",
    "                link = li.find(\"a\")[\"href\"]\n",
    "\n",
    "                # Append the data to the lists\n",
    "                list_title_hr.append(title)\n",
    "                list_link_hr.append(link)\n",
    "                list_source_hr.append(\"ITV_HR\")\n",
    "                list_time_hr.append(date1)\n",
    "                c = c + 1\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"ITV\", str(e))\n",
    "\n",
    "\n",
    "    # list_title_hr1\n",
    "\n",
    "    # list_link_hr1\n",
    "\n",
    "\n",
    "    url10 = \"https://www.ptcnews.tv/haryana-nation\"\n",
    "\n",
    "\n",
    "    try:\n",
    "        # Send a GET request to the website\n",
    "        r10 = requests.get(url10)\n",
    "\n",
    "        # Create a BeautifulSoup object to parse the HTML content\n",
    "        soup10 = BeautifulSoup(r10.content, \"html.parser\")\n",
    "\n",
    "        # Find the main container containing all the news articles\n",
    "        news_articles = soup10.find_all(\"h3\", class_=\"iw-news-title\")\n",
    "\n",
    "\n",
    "        # Iterate over the news articles and extract the headings and links\n",
    "        for article in news_articles:\n",
    "            title = article.text.strip()\n",
    "            link = article.find(\"a\")[\"href\"]\n",
    "            list_title_hr.append(title.strip())\n",
    "            list_link_hr.append(link)\n",
    "            list_source_hr.append(\"PTC_HR\")\n",
    "            list_time_hr.append(date1)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"PTCnews\", str(e))\n",
    "\n",
    "    # list_title_hr1\n",
    "\n",
    "\n",
    "    url10 = \"https://www.prabhatkhabar.com/state/haryana\"\n",
    "\n",
    "\n",
    "    try:\n",
    "        # Send a GET request to the website\n",
    "        r10 = requests.get(url10)\n",
    "\n",
    "        # Create a BeautifulSoup object to parse the HTML content\n",
    "        soup10 = BeautifulSoup(r10.content, \"html.parser\")\n",
    "\n",
    "        # Find the main container containing all the news articles\n",
    "        news_articles = soup10.find_all(\"div\", class_=\"arr--story-card storycard-m_card__KJwRa storycard-m_horizontal-card___7afz storycard-m_border-default__PEZ1x storycard-m_dark__2wCTq\")\n",
    "\n",
    "\n",
    "        # Iterate over the news articles and extract the headings and links\n",
    "        for article in news_articles:\n",
    "            title = article.text.strip()\n",
    "            link = article.find(\"a\")[\"href\"]\n",
    "            list_title_hr.append(title.strip())\n",
    "            list_link_hr.append(link)\n",
    "            list_source_hr.append(\"PK_HR\")\n",
    "            list_time_hr.append(date1)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"PKnews\", str(e))\n",
    "\n",
    "    # len(list_link_hr1)\n",
    "\n",
    "\n",
    "    url10 = \"https://www.livemint.com/topic/haryana\"\n",
    "\n",
    "\n",
    "    try:\n",
    "        # Send a GET request to the website\n",
    "        r10 = requests.get(url10)\n",
    "\n",
    "        # Create a BeautifulSoup object to parse the HTML content\n",
    "        soup10 = BeautifulSoup(r10.content, \"html.parser\")\n",
    "\n",
    "        # Find the main container containing all the news articles\n",
    "        news_articles = soup10.find_all(\"div\", class_=\"headlineSec\")\n",
    "\n",
    "\n",
    "        # Iterate over the news articles and extract the headings and links\n",
    "        for article in news_articles:\n",
    "            title = article.text.strip()\n",
    "            link = article.find(\"a\")[\"href\"]\n",
    "            list_title_hr.append(title.strip())\n",
    "            list_link_hr.append(link)\n",
    "            list_source_hr.append(\"LM_HR\")\n",
    "            list_time_hr.append(date1)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"LMnews\", str(e))\n",
    "\n",
    "    # list_title_hr1\n",
    "\n",
    "    url10 = \"https://www.livemint.com/topic/haryana\"\n",
    "\n",
    "\n",
    "    try:\n",
    "        # Send a GET request to the website\n",
    "        r10 = requests.get(url10)\n",
    "\n",
    "        # Create a BeautifulSoup object to parse the HTML content\n",
    "        soup10 = BeautifulSoup(r10.content, \"html.parser\")\n",
    "\n",
    "        # Find the main container containing all the news articles\n",
    "        news_articles = soup10.find_all(\"div\", class_=\"headlineSec\")\n",
    "\n",
    "\n",
    "        # Iterate over the news articles and extract the headings and links\n",
    "        for article in news_articles:\n",
    "            title = article.text.strip()\n",
    "            link = article.find(\"a\")[\"href\"]\n",
    "            list_title_hr.append(title.strip())\n",
    "            list_link_hr.append(link)\n",
    "            list_source_hr.append(\"LM_HR\")\n",
    "            list_time_hr.append(date1)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"LMnews\", str(e))\n",
    "\n",
    "    url10 = \"https://www.zeebiz.com/topics/haryana\"\n",
    "\n",
    "    try:\n",
    "        # Send a GET request to the website\n",
    "        r10 = requests.get(url10)\n",
    "\n",
    "        # Create a BeautifulSoup object to parse the HTML content\n",
    "        soup10 = BeautifulSoup(r10.content, \"html.parser\")\n",
    "\n",
    "        # Find the main container containing all the news articles\n",
    "        news_articles = soup10.find_all(\"div\", class_=\"mstrecntbx clearfix\")\n",
    "\n",
    "\n",
    "        # Iterate over the news articles and extract the headings and links\n",
    "        for article in news_articles:\n",
    "            title = article.text.strip()\n",
    "            link = article.find(\"a\")[\"href\"]\n",
    "            list_title_hr.append(title.strip())\n",
    "            list_link_hr.append(url10+link)\n",
    "            list_source_hr.append(\"ZeeBiz_HR\")\n",
    "            list_time_hr.append(date1)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"ZBnews\", str(e))\n",
    "\n",
    "    import requests\n",
    "    from bs4 import BeautifulSoup\n",
    "\n",
    "    url10 = \"https://www.tv9hindi.com/state/haryana\"\n",
    "    # You may need to set headers based on the website requirements\n",
    "\n",
    "\n",
    "    try:\n",
    "        # Send a GET request to the website with headers\n",
    "        r10 = requests.get(url10, headers=headers)\n",
    "\n",
    "        # Create a BeautifulSoup object to parse the HTML content\n",
    "        soup10 = BeautifulSoup(r10.content, \"html.parser\")\n",
    "\n",
    "        # Find the main container containing all the news articles\n",
    "        news_articles = soup10.find_all(\"figure\")\n",
    "\n",
    "        # Iterate over the news articles and extract the title and href\n",
    "        for article in news_articles:\n",
    "            title = article.find(\"a\")[\"title\"]\n",
    "            link = article.find(\"a\")[\"href\"]\n",
    "            list_title_hr.append(title.strip())\n",
    "            list_link_hr.append(link)\n",
    "            list_source_hr.append(\"TV9_HR\")\n",
    "            list_time_hr.append(date1)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"TV9news\", str(e))\n",
    "\n",
    "\n",
    "    import requests\n",
    "    from bs4 import BeautifulSoup\n",
    "\n",
    "    url10 = \"https://www.abplive.com/topic/haryana\"\n",
    "\n",
    "    try:\n",
    "        # Send a GET request to the website\n",
    "        r10 = requests.get(url10)\n",
    "\n",
    "        # Create a BeautifulSoup object to parse the HTML content\n",
    "        soup10 = BeautifulSoup(r10.content, \"html.parser\")\n",
    "\n",
    "        # Find the main container containing all the news articles\n",
    "        news_articles = soup10.find_all(\"a\", class_=\"topic_text\")\n",
    "\n",
    "        # Iterate over the news articles and extract the headings and links\n",
    "        for article in news_articles:\n",
    "            title = article.text.strip()\n",
    "            link = article.get(\"href\")\n",
    "            list_title_hr.append(title)\n",
    "            list_link_hr.append(link)\n",
    "            list_source_hr.append(\"ABP_HR\")\n",
    "            list_time_hr.append(date1)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"ABPnews\", str(e))\n",
    "\n",
    "    url10 = \"https://indianexpress.com/about/haryana/\"\n",
    "\n",
    "\n",
    "    try:\n",
    "        # Send a GET request to the website\n",
    "        r10 = requests.get(url10)\n",
    "\n",
    "        # Create a BeautifulSoup object to parse the HTML content\n",
    "        soup10 = BeautifulSoup(r10.content, \"html.parser\")\n",
    "\n",
    "        # Find the main container containing all the news articles\n",
    "        news_articles = soup10.find_all(\"div\", class_=\"img-context\")\n",
    "\n",
    "\n",
    "        # Iterate over the news articles and extract the headings and links\n",
    "        for article in news_articles:\n",
    "            title = article.text.strip()\n",
    "            link = article.find(\"a\")[\"href\"]\n",
    "            list_title_hr.append(title.strip())\n",
    "            list_link_hr.append(link)\n",
    "            list_source_hr.append(\"IE_HR\")\n",
    "            list_time_hr.append(date1)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"IEnews\", str(e))\n",
    "\n",
    "    # list_title_hr1\n",
    "\n",
    "    url10 = \"https://www.indiatvnews.com/topic/haryana\"\n",
    "\n",
    "\n",
    "    try:\n",
    "        # Send a GET request to the website\n",
    "        r10 = requests.get(url10)\n",
    "\n",
    "        # Create a BeautifulSoup object to parse the HTML content\n",
    "        soup10 = BeautifulSoup(r10.content, \"html.parser\")\n",
    "\n",
    "        # Find the main container containing all the news articles\n",
    "        news_articles = soup10.find_all(\"h3\", class_=\"title\")\n",
    "\n",
    "\n",
    "        # Iterate over the news articles and extract the headings and links\n",
    "        for article in news_articles:\n",
    "            title = article.text.strip()\n",
    "            link = article.find(\"a\")[\"href\"]\n",
    "            list_title_hr.append(title.strip())\n",
    "            list_link_hr.append(link)\n",
    "            list_source_hr.append(\"ITV_HR\")\n",
    "            list_time_hr.append(date1)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"ITVnews\", str(e))\n",
    "\n",
    "    # list_link_hr1\n",
    "    # list_title_hr1\n",
    "\n",
    "    url10 = \"https://www.ndtv.com/topic/haryana\"\n",
    "\n",
    "\n",
    "    try:\n",
    "        # Send a GET request to the website\n",
    "        r10 = requests.get(url10)\n",
    "\n",
    "        # Create a BeautifulSoup object to parse the HTML content\n",
    "        soup10 = BeautifulSoup(r10.content, \"html.parser\")\n",
    "\n",
    "        # Find the main container containing all the news articles\n",
    "        news_articles = soup10.find_all(\"div\", class_=\"src_itm-ttl\")\n",
    "\n",
    "\n",
    "        # Iterate over the news articles and extract the headings and links\n",
    "        for article in news_articles:\n",
    "            title = article.text.strip()\n",
    "            link = article.find(\"a\")[\"href\"]\n",
    "            list_title_hr.append(title.strip())\n",
    "            list_link_hr.append(link)\n",
    "            list_source_hr.append(\"NDTV_HR\")\n",
    "            list_time_hr.append(date1)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"NDTVnews\", str(e))\n",
    "\n",
    "    import requests\n",
    "    from bs4 import BeautifulSoup\n",
    "\n",
    "    url10 = \"https://www.amarujala.com/haryana\"\n",
    "    # You may need to set headers based on the website requirements\n",
    "    headers = {\n",
    "        \"User-Agent\": \"Your User Agent String\"\n",
    "    }\n",
    "\n",
    "\n",
    "    try:\n",
    "        # Send a GET request to the website with headers\n",
    "        r10 = requests.get(url10, headers=headers)\n",
    "\n",
    "        # Create a BeautifulSoup object to parse the HTML content\n",
    "        soup10 = BeautifulSoup(r10.content, \"html.parser\")\n",
    "\n",
    "        # Find the main container containing all the news articles\n",
    "        news_articles = soup10.find_all(\"div\", class_=\"image_description\")\n",
    "\n",
    "        # Iterate over the news articles and extract the title and href\n",
    "        for article in news_articles:\n",
    "            title = article.find(\"a\")[\"title\"]\n",
    "            link = article.find(\"a\")[\"href\"]\n",
    "            list_title_hr.append(title.strip())\n",
    "            list_link_hr.append(\"https://www.amarujala.com\"+link)\n",
    "            list_source_hr.append(\"AMJ_HR\")\n",
    "            list_time_hr.append(date1)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"AMJnews\", str(e))\n",
    "\n",
    "\n",
    "    import requests\n",
    "    from bs4 import BeautifulSoup\n",
    "\n",
    "    url10 = \"https://timesofindia.indiatimes.com/india/haryana\"\n",
    "    # You may need to set headers based on the website requirements\n",
    "    headers = {\n",
    "        \"User-Agent\": \"Your User Agent String\"\n",
    "    }\n",
    "\n",
    "\n",
    "\n",
    "    try:\n",
    "        # Send a GET request to the website with headers\n",
    "        r10 = requests.get(url10, headers=headers)\n",
    "\n",
    "        # Create a BeautifulSoup object to parse the HTML content\n",
    "        soup10 = BeautifulSoup(r10.content, \"html.parser\")\n",
    "\n",
    "        # Find the main container containing all the news articles\n",
    "        news_articles = soup10.find_all(\"span\", class_=\"w_tle\")\n",
    "\n",
    "        # Iterate over the news articles and extract the title and href\n",
    "        for article in news_articles:\n",
    "            title = article.find(\"a\")[\"title\"]\n",
    "            link = article.find(\"a\")[\"href\"]\n",
    "            list_title_hr.append(title.strip())\n",
    "            list_link_hr.append(\"https://timesofindia.indiatimes.com/india/haryana\"+link)\n",
    "            list_source_hr.append(\"TOI_HR\")\n",
    "            list_time_hr.append(date1)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"TOInews\", str(e))\n",
    "\n",
    "\n",
    "    # import requests\n",
    "    # from bs4 import BeautifulSoup\n",
    "\n",
    "    url10 = \"https://www.khaskhabar.com/state/haryana\"\n",
    "    # You may need to set headers based on the website requirements\n",
    "    headers = {\n",
    "        \"User-Agent\": \"Your User Agent String\"\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        # Send a GET request to the website with headers\n",
    "        r10 = requests.get(url10, headers=headers)\n",
    "\n",
    "        # Create a BeautifulSoup object to parse the HTML content\n",
    "        soup10 = BeautifulSoup(r10.content, \"html.parser\")\n",
    "\n",
    "        # Find the main container containing all the news articles\n",
    "        news_articles = soup10.find_all(\"li\", class_=\"withimg takemeleft\")\n",
    "\n",
    "        # Iterate over the news articles and extract the title and href\n",
    "        for article in news_articles:\n",
    "            title = article.find(\"a\")[\"title\"]\n",
    "            link = article.find(\"a\")[\"href\"]\n",
    "            list_title_hr.append(title.strip())\n",
    "            list_link_hr.append(link)\n",
    "            list_source_hr.append(\"KK_HR\")\n",
    "            list_time_hr.append(date1)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"KKnews\", str(e))\n",
    "\n",
    "\n",
    "    import requests\n",
    "    from bs4 import BeautifulSoup\n",
    "\n",
    "    url10 = \"https://hindi.news18.com/news/haryana/\"\n",
    "    # You may need to set headers based on the website requirements\n",
    "    headers = {\n",
    "        \"User-Agent\": \"Your User Agent String\"\n",
    "    }\n",
    "\n",
    "\n",
    "\n",
    "    try:\n",
    "        # Send a GET request to the website with headers\n",
    "        r10 = requests.get(url10, headers=headers)\n",
    "\n",
    "        # Create a BeautifulSoup object to parse the HTML content\n",
    "        soup10 = BeautifulSoup(r10.content, \"html.parser\")\n",
    "\n",
    "        # Find the main container containing all the news articles\n",
    "        news_articles = soup10.find_all(\"li\", class_=\"jsx-4d499163fac48ffc rltdnw\")\n",
    "\n",
    "        # Iterate over the news articles and extract the title and href\n",
    "        for article in news_articles:\n",
    "            title = article.find(\"h3\").text\n",
    "            link = article.find(\"a\")[\"href\"]\n",
    "            list_title_hr.append(title.strip())\n",
    "            list_link_hr.append(link)\n",
    "            list_source_hr.append(\"N18_HR\")\n",
    "            list_time_hr.append(date1)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"N18news\", str(e))\n",
    "\n",
    "\n",
    "    # list_title_hr\n",
    "    # import requests\n",
    "    # from bs4 import BeautifulSoup\n",
    "\n",
    "    url10 = \"https://www.indiatoday.in/india/haryana\"\n",
    "    # You may need to set headers based on the website requirements\n",
    "    headers = {\n",
    "        \"User-Agent\": \"Your User Agent String\"\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        # Send a GET request to the website with headers\n",
    "        r10 = requests.get(url10, headers=headers)\n",
    "\n",
    "        # Create a BeautifulSoup object to parse the HTML content\n",
    "        soup10 = BeautifulSoup(r10.content, \"html.parser\")\n",
    "\n",
    "        # Find the main container containing all the news articles\n",
    "        news_articles = soup10.find_all(\"article\", class_=\"B1S3_story__card__A_fhi\")\n",
    "\n",
    "        # Iterate over the news articles and extract the title and href\n",
    "        for article in news_articles:\n",
    "            title = article.find(\"a\")[\"title\"]\n",
    "            link = article.find(\"a\")[\"href\"]\n",
    "            list_title_hr.append(title.strip())\n",
    "            list_link_hr.append(\"https://www.indiatoday.in\" + link)\n",
    "            list_source_hr.append(\"IT_HR\")\n",
    "            list_time_hr.append(date1)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"ITnews\", str(e))\n",
    "\n",
    "\n",
    "    # list_link_hr\n",
    "    # import requests\n",
    "    # from bs4 import BeautifulSoup\n",
    "\n",
    "    url10 = \"https://www.deccanherald.com/india/haryana\"\n",
    "    # You may need to set headers based on the website requirements\n",
    "    headers = {\n",
    "        \"User-Agent\": \"Your User Agent String\"\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        # Send a GET request to the website with headers\n",
    "        r10 = requests.get(url10, headers=headers)\n",
    "\n",
    "        # Create a BeautifulSoup object to parse the HTML content\n",
    "        soup10 = BeautifulSoup(r10.content, \"html.parser\")\n",
    "\n",
    "        # Find the main container containing all the news articles\n",
    "        news_articles = soup10.find_all(\"div\", class_=\"-bdFE\")\n",
    "\n",
    "        # Iterate over the news articles and extract the title and href\n",
    "        for article in news_articles:\n",
    "            title = article.find(\"h1\", class_=\"headline\")  # Updated here\n",
    "            link = article.find(\"a\")[\"href\"]\n",
    "            if title:\n",
    "                title_text = title.text.strip()\n",
    "                list_title_hr.append(title_text)\n",
    "                list_link_hr.append(\"https://www.deccanherald.com\" + link)\n",
    "                list_source_hr.append(\"DH_HR\")\n",
    "                list_time_hr.append(date1)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"DHnews\", str(e))\n",
    "\n",
    "\n",
    "    # len(list_link_hr)\n",
    "\n",
    "    # len(list_link_hr)\n",
    "    import requests\n",
    "    from bs4 import BeautifulSoup\n",
    "\n",
    "    url10 = \"https://zeenews.india.com/hindi/tags/haryana.html\"\n",
    "    # You may need to set headers based on the website requirements\n",
    "    headers = {\n",
    "        \"User-Agent\": \"Your User Agent String\"\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        # Send a GET request to the website with headers\n",
    "        r10 = requests.get(url10, headers=headers)\n",
    "\n",
    "        # Create a BeautifulSoup object to parse the HTML content\n",
    "        soup10 = BeautifulSoup(r10.content, \"html.parser\")\n",
    "\n",
    "        # Find the main container containing all the news articles\n",
    "        news_articles = soup10.find_all(\"div\", class_=\"news_left\")\n",
    "\n",
    "        # Iterate over the news articles and extract the title and href\n",
    "        for article in news_articles:\n",
    "            title = article.find(\"div\", class_=\"news_title\").find(\"a\")[\"title\"]\n",
    "            link = article.find(\"div\", class_=\"news_title\").find(\"a\")[\"href\"]\n",
    "            list_title_hr.append(title.strip())\n",
    "            list_link_hr.append(\"https://zeenews.india.com\" + link)\n",
    "            list_source_hr.append(\"ZEE_HR\")\n",
    "            list_time_hr.append(date1)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"ZEEnews\", str(e))\n",
    "\n",
    "\n",
    "    # list_link_hr\n",
    "\n",
    "    # import requests\n",
    "    # from bs4 import BeautifulSoup\n",
    "\n",
    "    url10 = \"https://www.indiatoday.in/topic/haryana\"\n",
    "    # You may need to set headers based on the website requirements\n",
    "    headers = {\n",
    "        \"User-Agent\": \"Your User Agent String\"\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        # Send a GET request to the website with headers\n",
    "        r10 = requests.get(url10, headers=headers)\n",
    "\n",
    "        # Create a BeautifulSoup object to parse the HTML content\n",
    "        soup10 = BeautifulSoup(r10.content, \"html.parser\")\n",
    "\n",
    "        # Find the main container containing all the news articles\n",
    "        news_articles = soup10.find_all(\"article\", class_=\"B1S3_story__card__A_fhi\")\n",
    "\n",
    "        # Iterate over the news articles and extract the title and href\n",
    "    #     list_title_hr = []\n",
    "    #     list_link_hr = []\n",
    "    #     list_source_hr = []\n",
    "    #     list_time_hr = []\n",
    "\n",
    "        for article in news_articles:\n",
    "            title = article.find(\"h3\").find(\"a\")[\"title\"]\n",
    "            link = article.find(\"h3\").find(\"a\")[\"href\"]\n",
    "            list_title_hr.append(title.strip())\n",
    "            list_link_hr.append(link)\n",
    "            list_source_hr.append(\"IT_HR\")\n",
    "            list_time_hr.append(date1)\n",
    "            # You need to add the code to get the date information and assign it to the 'date1' variable.\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"ITnews\", str(e))\n",
    "\n",
    "\n",
    "    import requests\n",
    "    from bs4 import BeautifulSoup\n",
    "\n",
    "    url10 = \"https://www.ndtv.com/topic/haryana\"\n",
    "    # You may need to set headers based on the website requirements\n",
    "    headers = {\n",
    "        \"User-Agent\": \"Your User Agent String\"\n",
    "    }\n",
    "\n",
    "    # list_title_hr = []\n",
    "    # list_link_hr = []\n",
    "    # list_source_hr = []\n",
    "    # list_time_hr = []\n",
    "\n",
    "    try:\n",
    "        # Send a GET request to the website with headers\n",
    "        r10 = requests.get(url10, headers=headers)\n",
    "\n",
    "        # Create a BeautifulSoup object to parse the HTML content\n",
    "        soup10 = BeautifulSoup(r10.content, \"html.parser\")\n",
    "\n",
    "        # Find the main container containing all the news articles\n",
    "        news_articles = soup10.find_all(\"li\", class_=\"src_lst-li\")\n",
    "\n",
    "        # Iterate over the news articles and extract the title and href\n",
    "        for article in news_articles:\n",
    "            title = article.find(\"div\", class_=\"src_itm-ttl\").find(\"a\")[\"title\"].replace(\"&quot;\", \"\")\n",
    "            link = article.find(\"div\", class_=\"src_itm-ttl\").find(\"a\")[\"href\"]\n",
    "            list_title_hr.append(title.strip())\n",
    "            list_link_hr.append(link)\n",
    "            list_source_hr.append(\"NDTV_HR\")\n",
    "            # Assuming you have the 'date1' variable defined somewhere in your code\n",
    "            list_time_hr.append(date1)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"NDTVnews\", str(e))\n",
    "\n",
    "\n",
    "    import requests\n",
    "    from bs4 import BeautifulSoup\n",
    "\n",
    "    url10 = \"https://www.business-standard.com/topic/haryana\"\n",
    "    # You may need to set headers based on the website requirements\n",
    "    headers = {\n",
    "        \"User-Agent\": \"Your User Agent String\"\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        # Send a GET request to the website with headers\n",
    "        r10 = requests.get(url10, headers=headers)\n",
    "\n",
    "        # Create a BeautifulSoup object to parse the HTML content\n",
    "        soup10 = BeautifulSoup(r10.content, \"html.parser\")\n",
    "\n",
    "        # Find the main container containing all the news articles\n",
    "        news_articles = soup10.find_all(\"div\", class_=\"cardlist\")\n",
    "\n",
    "        # Iterate over the news articles and extract the title and href\n",
    "        for article in news_articles:\n",
    "            title_element = article.find(\"a\", class_=\"smallcard-title\")\n",
    "            link_element = article.find(\"a\", class_=\"smallcard-title\")\n",
    "\n",
    "            # Check if both title and link elements are found\n",
    "            if title_element and link_element:\n",
    "                title = title_element.text.strip()\n",
    "                link = link_element[\"href\"]\n",
    "\n",
    "                # Append to lists if needed\n",
    "                list_title_hr.append(title)\n",
    "                list_link_hr.append(link)\n",
    "                list_source_hr.append(\"BS_HR\")\n",
    "                list_time_hr.append(date1)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"BSnews\", str(e))\n",
    "\n",
    "\n",
    "    print(len(list_time_hr))\n",
    "    print(len(list_source_hr))\n",
    "    print(len(list_link_hr))\n",
    "    print(len(list_title_hr))\n",
    "\n",
    "    df_hr = pd.DataFrame(list(zip(list_source_hr,list_title_hr,list_link_hr,list_time_hr)),columns = ['source','titles','link','Time'])\n",
    "\n",
    "\n",
    "\n",
    "    df_hr=df_hr.dropna()\n",
    "    df_hr=df_hr[df_hr['titles']!='']\n",
    "    df_hr.drop_duplicates(subset=['link'],keep=False,inplace=True)\n",
    "    connection_url = 'postgresql://mindshare:mindshare@postgresql-76953-0.cloudclusters.net:19477/postgres'\n",
    "    engine = create_engine(connection_url)\n",
    "    connection = engine.connect()\n",
    "\n",
    "    dump_hrdf = pd.read_sql_table('hrnews_dump_table', connection)\n",
    "\n",
    "\n",
    "    compare_list=dump_hrdf['link'].unique().tolist()\n",
    "#     hr_df=df_hr.copy()\n",
    "    hr_df = df_hr[~df_hr[\"link\"].isin(compare_list)]\n",
    "\n",
    "\n",
    "    df_hr.shape\n",
    "\n",
    "    hr_df.shape\n",
    "\n",
    "    translator = Translator()\n",
    "    # hr_df=df_hr.copy()\n",
    "    # Function to translate text to English\n",
    "    def translate_to_english(text):\n",
    "        try:\n",
    "            translation = translator.translate(text, src='auto', dest='en')\n",
    "            return translation.text\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            return text\n",
    "\n",
    "    # Apply translation function to the DataFrame and create a new column\n",
    "    hr_df['titles'] = hr_df['titles'].apply(translate_to_english)\n",
    "\n",
    "    hr_df=hr_df.dropna()\n",
    "    hr_df.drop_duplicates(keep=False,inplace=True)\n",
    "    hr_df=hr_df[hr_df['titles']!='']    \n",
    "    # return hr_df\n",
    "    # ll=['https://www.tribuneindia.com/news/haryana/keep-three-wheelers-off-e-way-537721','https://www.tribuneindia.com/news/haryana/man-held-with-banned-pills-537718']\n",
    "    # hr_df=hr_df[~hr_df['link'].isin(ll)]\n",
    "    ###################################################################################\n",
    "    connection_url = 'postgresql://mindshare:mindshare@postgresql-76953-0.cloudclusters.net:19477/postgres'\n",
    "    engine = create_engine(connection_url)\n",
    "    connection = engine.connect()\n",
    "    hr_df.to_sql('hrnews_dump_table',con=connection, if_exists='append',index=False)\n",
    "\n",
    "    hr_df.to_excel('Haryana_Latest_News_'+date1+'.xlsx')\n",
    "\n",
    "\n",
    "    # path='C:\\Users\\minds\\OneDrive\\Desktop\\work amar DND\\codes\\scrapping\\snscrape\\Untitled Folder 3\\TS voter list'\n",
    "\n",
    "    # hr_df.to_excel('haryana_df_aaj.xlsx')\n",
    "    hr_df['source'].value_counts()\n",
    "\n",
    "    # date1=datetime.now()\n",
    "    # date1 = date1.strftime(\"%d-%m-%Y %H-%M-%S\")\n",
    "    print(hr_df.shape)\n",
    "\n",
    "    hr_df.to_excel('Haryana_Latest_News_'+date1+'.xlsx')\n",
    "\n",
    "    date1=datetime.now()\n",
    "    date1 = date1.strftime(\"%d-%m-%Y %H-%M-%S\")\n",
    "    workbook = xlsxwriter.Workbook('Haryana_'+date1+'_mediatrackerexcel.xlsx')\n",
    "    workbook.use_zip64()\n",
    "\n",
    "\n",
    "    full_border3  = workbook.add_format({'border_color':\"#000000\",'align':'left','font_color':'blue','font_size':20})\n",
    "    df=hr_df.copy()\n",
    "    worksheet = workbook.add_worksheet()\n",
    "    title = 'Media Tracker Report'\n",
    "    worksheet.write('A1',title,full_border3)\n",
    "    date = date.today()\n",
    "    df = df[df['link'].notna()]\n",
    "    df = df[df['titles'].notna()]\n",
    "\n",
    "    k =1\n",
    "    row =4\n",
    "\n",
    "    header_format = workbook.add_format({'bold': True,'text_wrap': True,'align':'center','valign': 'top','fg_color': '#85C9E8','border': 1})\n",
    "    col = ['Article','Link to Article']\n",
    "    s =0\n",
    "\n",
    "    worksheet.set_column(0, 0, 62)\n",
    "    worksheet.set_column(1, 1, 24)\n",
    "\n",
    "\n",
    "    for value in col:\n",
    "        worksheet.write(3, s , value, header_format)\n",
    "        s = s+1\n",
    "    full_border1  = workbook.add_format({'border':1,'border_color':\"#000000\",'align':'left'})\n",
    "    full_border2  = workbook.add_format({'border':1,'border_color':\"#000000\",'text_wrap':True})\n",
    "\n",
    "    for item,value in df.iterrows():\n",
    "        worksheet.write(row,0,value['titles'],full_border1)\n",
    "        worksheet.write_url(row ,1,value['link'],full_border2,string = value['source'])\n",
    "\n",
    "        row = row+1\n",
    "    workbook.close()\n",
    "\n",
    "    app = client.DispatchEx(\"Excel.Application\")\n",
    "\n",
    "\n",
    "    sheets = app.Workbooks.Open(r'C:\\Users\\minds\\OneDrive\\Desktop\\work amar DND\\codes\\scrapping\\snscrape\\Untitled Folder 3\\TS voter list\\Haryana_'+date1+'_mediatrackerexcel.xlsx')\n",
    "    sheets.ExportAsFixedFormat(0, r'C:\\Users\\minds\\OneDrive\\Desktop\\work amar DND\\codes\\scrapping\\snscrape\\Untitled Folder 3\\TS voter list\\Haryana_'+date1+'_mediatrackerexcel.pdf')\n",
    "\n",
    "    # email_from = 'mugilan.deiveegan@themindshare.in'\n",
    "    # email_from = 'abhijeet.yadav@themindshare.i'\n",
    "    # password = 'Abhij33t@1997'\n",
    "    # email_to = ['abhijeet.yadav@themindshare.in','shikhar.amar@themindshare.in']\n",
    "    # email_to = ['abhijeet.yadav@themindshare.in']\n",
    "    email_from = 'shikhar.amar@themindshare.in'\n",
    "    password = 'Inclusive@1997'\n",
    "#     email_to = ['shikhar.amar@themindshare.in']\n",
    "    email_to = ['shikhar.amar@themindshare.in','nitish15060@gmail.com','sudhir.Chaudhary@themindshare.in','bijender.Sheoran@themindshare.in ']\n",
    "    # email_to = ['shikhar.amar@themindshare.in','rahul.sunny@themindshare.in','abhijeet.yadav@themindshare.in','hanok.martin@themindshare.in','mp@themindshare.in','akshayarun.joshi@themindshare.in','divyanshu.divyam@themindshare.in']\n",
    "\n",
    "    date_str = pd.Timestamp.today().strftime('%Y-%m-%d')\n",
    "\n",
    "    email_message = MIMEMultipart()\n",
    "    email_message['From'] = email_from\n",
    "    email_message['To'] = \", \".join(email_to)\n",
    "    email_message['Subject'] = f'Haryana Media Tracker Report Email - {date1}'\n",
    "    text = ''\n",
    "\n",
    "    part = MIMEBase('application', \"octet-stream\")\n",
    "    part.set_payload(open(r'C:\\Users\\minds\\OneDrive\\Desktop\\work amar DND\\codes\\scrapping\\snscrape\\Untitled Folder 3\\TS voter list\\Haryana_'+date1+'_mediatrackerexcel.pdf', \"rb\").read())\n",
    "    encoders.encode_base64(part)\n",
    "    part.add_header('Content-Disposition', 'attachment; filename=\"'r'Haryana_'+date1+'_mediatrackerexcel.pdf\"')\n",
    "    email_message.attach(part)\n",
    "\n",
    "\n",
    "    email_message.attach(MIMEText(text))\n",
    "    # Convert it as a string\n",
    "    email_string = email_message.as_string()\n",
    "\n",
    "    context = ssl.create_default_context()\n",
    "    with smtplib.SMTP_SSL(\"smtp.gmail.com\", 465, context=context) as server:\n",
    "        server.login(email_from, password)\n",
    "        server.sendmail(email_from, email_to, email_string)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f575e9cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import schedule\n",
    "import time\n",
    "# chhattisgarh_media_tracker()\n",
    "schedule.every().day.at('08:00').do(haryana_news)\n",
    "\n",
    "\n",
    "while True:\n",
    "    schedule.run_pending()\n",
    "    time.sleep(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a1ec7b68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1716\n",
      "1716\n",
      "1716\n",
      "1716\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\minds\\AppData\\Local\\Temp\\ipykernel_22056\\4073582643.py:1101: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  hr_df['titles'] = hr_df['titles'].apply(translate_to_english)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(280, 4)\n"
     ]
    }
   ],
   "source": [
    "haryana_news()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc7df656",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4779752c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a3e51d3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a544310",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3f9c42b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfb895e8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e3703a4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
