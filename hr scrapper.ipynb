def haryana_news():
    import requests
    import queue
    # import telegram
    from bs4 import BeautifulSoup
    # import telegram.ext
    # from telegram.ext import Updater, CommandHandler
    import requests
    from bs4 import BeautifulSoup
    import numpy as np
    import pandas as pd
    import html5lib
    import lxml
    import xlsxwriter
    from datetime import date
    import googletrans
    from googletrans import Translator
    import smtplib, ssl
    from smtplib import SMTP_SSL
    from email.mime.multipart import MIMEMultipart
    from email.mime.text import MIMEText
    from email.mime.base import MIMEBase
    from email import encoders
    from win32com import client
    from datetime import datetime
    import schedule
    import time
    from datetime import date
    from datetime import timedelta
    import textwrap
    import psycopg2
    import sqlalchemy
    from sqlalchemy import create_engine, MetaData, Table, Column, Integer, String
    import selenium
    from selenium import webdriver
    from selenium.webdriver.chrome.options import Options
    from webdriver_manager.chrome import ChromeDriverManager
    from selenium.webdriver.common.keys import Keys
    from selenium.webdriver.common.by import By
    from selenium.webdriver.support.ui import WebDriverWait
    from selenium.common.exceptions import ElementClickInterceptedException
    from selenium.webdriver.chrome.options import Options
    #import pyshorteners
    import tempfile

    list_title_hr=[]
    list_link_hr=[]
    list_source_hr=[]
    list_time_hr = []

    date1=datetime.now()
    date1 = date1.strftime("%d-%m-%Y %H-%M-%S")

    
    url7 = "https://www.hindustantimes.com/cities/chandigarh-news"
    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36"
    }
    
    try:
        r7 = requests.get(url7, headers = headers)

        soup7 = BeautifulSoup(r7.content, "html.parser")

        news_articles = soup7.find_all("h3", class_="hdg3")

        for article in news_articles:
            title = article.text.strip()
            link = article.find("a")["href"]
            list_title_hr.append(title.strip())
            list_link_hr.append('https://www.hindustantimes.com'+link)
            list_source_hr.append("HT_HR&PB")
            list_time_hr.append(date1)

    except Exception as e:
        print("HThrpb", str(e))


    url8 = "https://haryana.punjabkesari.in/"
    

    try:
        r8 = requests.get(url8)

        soup8 = BeautifulSoup(r8.content, "html.parser")

        news_articles = soup8.find("ul", class_="pargrap-n pr-4").find_all("li")


        for article in news_articles:
            title = article.find("h3").text.strip()
            link = article.find("a")["href"]
            list_title_hr.append(title.strip())
            list_link_hr.append(link)
            list_source_hr.append("PK_HR")
            list_time_hr.append(date1)

    except Exception as e:
        print("PK", str(e))


    url9 = "https://www.haribhoomi.com/local/haryana"
        

    for i in range(1,4):
        try:
            r9 = requests.get('https://www.haribhoomi.com/local/haryana/'+str(i)+'')

            soup9 = BeautifulSoup(r9.content, "html.parser")

            news_articles = soup9.find_all("div", class_="list_content")


            for article in news_articles:
                title = article.text.strip()
                link = article.find("a")["href"]
                list_title_hr.append(title.strip())
                list_link_hr.append('https://www.haribhoomi.com/'+link)
                list_source_hr.append("hb_HR")
                list_time_hr.append(date1)

        except Exception as e:
            print("HariBhoomi HR", str(e))


    Zeenewsurl9 = ["rewari","chandigarh","sonipat","panipat","bhiwani","karnal",
                    "kurukshetra","rohtak","jhajjar","faridabad","yamuna-nagar","narnaul","fatehabad","mahendragarh","sirsa","palwal"]
    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36"
    }

    for i in Zeenewsurl9:
        try:
            r9 = requests.get('https://zeenews.india.com/hindi/india/delhi-ncr-haryana/'+str(i)+'', headers=headers)

            soup9 = BeautifulSoup(r9.content, "html.parser")

            news_articles = soup9.find_all("div", class_="news_title")


            for article in news_articles:
                title = article.text.strip()
                link = article.find("a")["href"]
                list_title_hr.append(title.strip())
                list_link_hr.append('https://zeenews.india.com'+link)
                list_source_hr.append("ZN_HR")
                list_time_hr.append(date1)

        except Exception as e:
            print("ZNhr", str(e))


    url10 = "https://www.indianewsharyana.com/haryana/"


    try:
        r10 = requests.get(url10)

        soup10 = BeautifulSoup(r10.content, "html.parser")

        news_articles = soup10.find_all("h3", class_="entry-title td-module-title")


        for article in news_articles:
            title = article.text.strip()
            link = article.find("a")["href"]
            list_title_hr.append(title.strip())
            list_link_hr.append(link)
            list_source_hr.append("INH_HR")
            list_time_hr.append(date1)

    except Exception as e:
        print("INH", str(e))

    url10 = "https://www.bhaskar.com/local/haryana/"

    try:
        r10 = requests.get(url10)

        soup10 = BeautifulSoup(r10.content, "html.parser")

        news_articles = soup10.find_all("li", class_="c7ff6507 db9a2680")


        for article in news_articles:
            title = article.text.strip()
            link = article.find("a")["href"]
            list_title_hr.append(title.strip())
            list_link_hr.append("https://www.bhaskar.com/"+link)
            list_source_hr.append("BH_HR")
            list_time_hr.append(date1)

    except Exception as e:
        print("BH", str(e))



    url10 = "https://www.jagran.com/haryana"


    try:
        r10 = requests.get(url10)

        soup10 = BeautifulSoup(r10.content, "html.parser")

        news_articles = soup10.find_all("li", class_="Editorial_CardStory__AOJMS CardStory")


        for article in news_articles:
            title = article.text.strip()
            link = article.find("a")["href"]
            list_title_hr.append(title.strip())
            list_link_hr.append("https://www.jagran.com/"+link)
            list_source_hr.append("JG_HR")
            list_time_hr.append(date1)

    except Exception as e:
        print("JG", str(e))



    url9 = "https://www.jagran.com/haryana"

    ll=['rohtak','hisar','panipat','yamunanagar','panchkula']

    for i in ll:
        try:
            r9 = requests.get('https://www.jagran.com/haryana/'+str(i)+'')

            soup9 = BeautifulSoup(r9.content, "html.parser")

            news_articles = soup9.find_all("li", class_="ListingSide_CardStory__weOJf CardStory")


            for article in news_articles:
                title = article.text.strip()
                link = article.find("a")["href"]
                list_title_hr.append(title.strip())
                list_link_hr.append(link)
                list_source_hr.append("JG_HR")
                list_time_hr.append(date1)

        except Exception as e:
            print("JG HR", str(e))

    # len(list_title_hr1)

    # list_title_hr1

    url10 = "https://www.aajtak.in/india/haryana"


    try:
        r10 = requests.get(url10)

        soup10 = BeautifulSoup(r10.content, "html.parser")

        news_articles = soup10.find_all("div", class_="widget-listing-content-section")


        for article in news_articles:
            title = article.text.strip()
            link = article.find("a")["href"]
            list_title_hr.append(title.strip())
            list_link_hr.append(link)
            list_source_hr.append("AAJ_HR")
            list_time_hr.append(date1)

    except Exception as e:
        print("AAJ", str(e))



    url10 = "https://haryana.indianews.in/#"


    try:
        r10 = requests.get(url10)

        soup10 = BeautifulSoup(r10.content, "html.parser")
        news_articles = soup10.find_all("h3", class_="entry-title td-module-title")

        for article in news_articles:
            title = article.text.strip()
            link = article.find("a")["href"]
            list_title_hr.append(title.strip())
            list_link_hr.append(link)
            list_source_hr.append("INDnews_HR")
            list_time_hr.append(date1)

    except Exception as e:
        print("INDnews", str(e))


    import requests
    from bs4 import BeautifulSoup

    url10 = "https://www.indiatv.in/haryana"


    try:
        r10 = requests.get(url10)
        soup10 = BeautifulSoup(r10.content, "html.parser")
        news_articles = soup10.find_all("ul", class_="list")

        c = 0
        for article in news_articles:
            li_elements = article.find_all("li")

            for li in li_elements:
                title = li.text.strip()
                link = li.find("a")["href"]
                list_title_hr.append(title)
                list_link_hr.append(link)
                list_source_hr.append("ITV_HR")
                list_time_hr.append(date1)
                c = c + 1

    except Exception as e:
        print("ITV", str(e))


    # list_title_hr1

    # list_link_hr1


    url10 = "https://www.ptcnews.tv/haryana-nation"


    try:
        r10 = requests.get(url10)

        soup10 = BeautifulSoup(r10.content, "html.parser")
        news_articles = soup10.find_all("h3", class_="iw-news-title")

        for article in news_articles:
            title = article.text.strip()
            link = article.find("a")["href"]
            list_title_hr.append(title.strip())
            list_link_hr.append(link)
            list_source_hr.append("PTC_HR")
            list_time_hr.append(date1)

    except Exception as e:
        print("PTCnews", str(e))

    # list_title_hr1


    url10 = "https://www.prabhatkhabar.com/state/haryana"


    try:
        r10 = requests.get(url10)
        soup10 = BeautifulSoup(r10.content, "html.parser")

        news_articles = soup10.find_all("div", class_="arr--story-card storycard-m_card__KJwRa storycard-m_horizontal-card___7afz storycard-m_border-default__PEZ1x storycard-m_dark__2wCTq")


        for article in news_articles:
            title = article.text.strip()
            link = article.find("a")["href"]
            list_title_hr.append(title.strip())
            list_link_hr.append(link)
            list_source_hr.append("PK_HR")
            list_time_hr.append(date1)

    except Exception as e:
        print("PKnews", str(e))

    # len(list_link_hr1)


    url10 = "https://www.livemint.com/topic/haryana"


    try:
        r10 = requests.get(url10)
        soup10 = BeautifulSoup(r10.content, "html.parser")
        news_articles = soup10.find_all("div", class_="headlineSec")
        for article in news_articles:
            title = article.text.strip()
            link = article.find("a")["href"]
            list_title_hr.append(title.strip())
            list_link_hr.append(link)
            list_source_hr.append("LM_HR")
            list_time_hr.append(date1)

    except Exception as e:
        print("LMnews", str(e))


    url10 = "https://www.livemint.com/topic/haryana"


    try:
        r10 = requests.get(url10)

        soup10 = BeautifulSoup(r10.content, "html.parser")

        news_articles = soup10.find_all("div", class_="headlineSec")


        for article in news_articles:
            title = article.text.strip()
            link = article.find("a")["href"]
            list_title_hr.append(title.strip())
            list_link_hr.append(link)
            list_source_hr.append("LM_HR")
            list_time_hr.append(date1)

    except Exception as e:
        print("LMnews", str(e))

    url10 = "https://www.zeebiz.com/topics/haryana"

    try:
        r10 = requests.get(url10)

        soup10 = BeautifulSoup(r10.content, "html.parser")

        news_articles = soup10.find_all("div", class_="mstrecntbx clearfix")


        for article in news_articles:
            title = article.text.strip()
            link = article.find("a")["href"]
            list_title_hr.append(title.strip())
            list_link_hr.append(url10+link)
            list_source_hr.append("ZeeBiz_HR")
            list_time_hr.append(date1)

    except Exception as e:
        print("ZBnews", str(e))

    import requests
    from bs4 import BeautifulSoup

    url10 = "https://www.tv9hindi.com/state/haryana"


    try:
        r10 = requests.get(url10, headers=headers)

        soup10 = BeautifulSoup(r10.content, "html.parser")

        news_articles = soup10.find_all("figure")

        for article in news_articles:
            title = article.find("a")["title"]
            link = article.find("a")["href"]
            list_title_hr.append(title.strip())
            list_link_hr.append(link)
            list_source_hr.append("TV9_HR")
            list_time_hr.append(date1)

    except Exception as e:
        print("TV9news", str(e))


    import requests
    from bs4 import BeautifulSoup

    url10 = "https://www.abplive.com/topic/haryana"

    try:
        r10 = requests.get(url10)

        soup10 = BeautifulSoup(r10.content, "html.parser")

        news_articles = soup10.find_all("a", class_="topic_text")

        for article in news_articles:
            title = article.text.strip()
            link = article.get("href")
            list_title_hr.append(title)
            list_link_hr.append(link)
            list_source_hr.append("ABP_HR")
            list_time_hr.append(date1)

    except Exception as e:
        print("ABPnews", str(e))

    url10 = "https://indianexpress.com/about/haryana/"


    try:
        r10 = requests.get(url10)

        soup10 = BeautifulSoup(r10.content, "html.parser")

        news_articles = soup10.find_all("div", class_="img-context")
        for article in news_articles:
            title = article.text.strip()
            link = article.find("a")["href"]
            list_title_hr.append(title.strip())
            list_link_hr.append(link)
            list_source_hr.append("IE_HR")
            list_time_hr.append(date1)

    except Exception as e:
        print("IEnews", str(e))

    # list_title_hr1

    url10 = "https://www.indiatvnews.com/topic/haryana"


    try:
        r10 = requests.get(url10)

        soup10 = BeautifulSoup(r10.content, "html.parser")

        news_articles = soup10.find_all("h3", class_="title")


        for article in news_articles:
            title = article.text.strip()
            link = article.find("a")["href"]
            list_title_hr.append(title.strip())
            list_link_hr.append(link)
            list_source_hr.append("ITV_HR")
            list_time_hr.append(date1)

    except Exception as e:
        print("ITVnews", str(e))

    # list_link_hr1
    # list_title_hr1

    url10 = "https://www.ndtv.com/topic/haryana"


    try:
        r10 = requests.get(url10)

        soup10 = BeautifulSoup(r10.content, "html.parser")

        news_articles = soup10.find_all("div", class_="src_itm-ttl")


        for article in news_articles:
            title = article.text.strip()
            link = article.find("a")["href"]
            list_title_hr.append(title.strip())
            list_link_hr.append(link)
            list_source_hr.append("NDTV_HR")
            list_time_hr.append(date1)

    except Exception as e:
        print("NDTVnews", str(e))

    import requests
    from bs4 import BeautifulSoup

    url10 = "https://www.amarujala.com/haryana"
    headers = {
        "User-Agent": "Your User Agent String"
    }


    try:
        r10 = requests.get(url10, headers=headers)

        soup10 = BeautifulSoup(r10.content, "html.parser")

        news_articles = soup10.find_all("div", class_="image_description")

        for article in news_articles:
            title = article.find("a")["title"]
            link = article.find("a")["href"]
            list_title_hr.append(title.strip())
            list_link_hr.append("https://www.amarujala.com"+link)
            list_source_hr.append("AMJ_HR")
            list_time_hr.append(date1)

    except Exception as e:
        print("AMJnews", str(e))


    import requests
    from bs4 import BeautifulSoup

    url10 = "https://timesofindia.indiatimes.com/india/haryana"
    headers = {
        "User-Agent": "Your User Agent String"
    }



    try:
        r10 = requests.get(url10, headers=headers)

        soup10 = BeautifulSoup(r10.content, "html.parser")

        news_articles = soup10.find_all("span", class_="w_tle")

        for article in news_articles:
            title = article.find("a")["title"]
            link = article.find("a")["href"]
            list_title_hr.append(title.strip())
            list_link_hr.append("https://timesofindia.indiatimes.com/india/haryana"+link)
            list_source_hr.append("TOI_HR")
            list_time_hr.append(date1)

    except Exception as e:
        print("TOInews", str(e))


    # import requests
    # from bs4 import BeautifulSoup

    url10 = "https://www.khaskhabar.com/state/haryana"
    headers = {
        "User-Agent": "Your User Agent String"
    }

    try:
        r10 = requests.get(url10, headers=headers)

        soup10 = BeautifulSoup(r10.content, "html.parser")

        news_articles = soup10.find_all("li", class_="withimg takemeleft")

        for article in news_articles:
            title = article.find("a")["title"]
            link = article.find("a")["href"]
            list_title_hr.append(title.strip())
            list_link_hr.append(link)
            list_source_hr.append("KK_HR")
            list_time_hr.append(date1)

    except Exception as e:
        print("KKnews", str(e))


    import requests
    from bs4 import BeautifulSoup

    url10 = "https://hindi.news18.com/news/haryana/"
    headers = {
        "User-Agent": "Your User Agent String"
    }



    try:
        r10 = requests.get(url10, headers=headers)

        soup10 = BeautifulSoup(r10.content, "html.parser")

        news_articles = soup10.find_all("li", class_="jsx-4d499163fac48ffc rltdnw")

        for article in news_articles:
            title = article.find("h3").text
            link = article.find("a")["href"]
            list_title_hr.append(title.strip())
            list_link_hr.append(link)
            list_source_hr.append("N18_HR")
            list_time_hr.append(date1)

    except Exception as e:
        print("N18news", str(e))


    # list_title_hr
    # import requests
    # from bs4 import BeautifulSoup

    url10 = "https://www.indiatoday.in/india/haryana"
    headers = {
        "User-Agent": "Your User Agent String"
    }

    try:
        r10 = requests.get(url10, headers=headers)

        soup10 = BeautifulSoup(r10.content, "html.parser")

        news_articles = soup10.find_all("article", class_="B1S3_story__card__A_fhi")

        for article in news_articles:
            title = article.find("a")["title"]
            link = article.find("a")["href"]
            list_title_hr.append(title.strip())
            list_link_hr.append("https://www.indiatoday.in" + link)
            list_source_hr.append("IT_HR")
            list_time_hr.append(date1)

    except Exception as e:
        print("ITnews", str(e))



    url10 = "https://www.deccanherald.com/india/haryana"
    headers = {
        "User-Agent": "Your User Agent String"
    }

    try:
        r10 = requests.get(url10, headers=headers)

        soup10 = BeautifulSoup(r10.content, "html.parser")

        news_articles = soup10.find_all("div", class_="-bdFE")

        for article in news_articles:
            title = article.find("h1", class_="headline")  
            link = article.find("a")["href"]
            if title:
                title_text = title.text.strip()
                list_title_hr.append(title_text)
                list_link_hr.append("https://www.deccanherald.com" + link)
                list_source_hr.append("DH_HR")
                list_time_hr.append(date1)

    except Exception as e:
        print("DHnews", str(e))


    # len(list_link_hr)

    # len(list_link_hr)
    import requests
    from bs4 import BeautifulSoup

    url10 = "https://zeenews.india.com/hindi/tags/haryana.html"
    headers = {
        "User-Agent": "Your User Agent String"
    }

    try:
        r10 = requests.get(url10, headers=headers)

        soup10 = BeautifulSoup(r10.content, "html.parser")

        news_articles = soup10.find_all("div", class_="news_left")

        for article in news_articles:
            title = article.find("div", class_="news_title").find("a")["title"]
            link = article.find("div", class_="news_title").find("a")["href"]
            list_title_hr.append(title.strip())
            list_link_hr.append("https://zeenews.india.com" + link)
            list_source_hr.append("ZEE_HR")
            list_time_hr.append(date1)

    except Exception as e:
        print("ZEEnews", str(e))


    
    url10 = "https://www.indiatoday.in/topic/haryana"
    headers = {
        "User-Agent": "Your User Agent String"
    }

    try:
        r10 = requests.get(url10, headers=headers)

        soup10 = BeautifulSoup(r10.content, "html.parser")

        news_articles = soup10.find_all("article", class_="B1S3_story__card__A_fhi")

    #     list_title_hr = []
    #     list_link_hr = []
    #     list_source_hr = []
    #     list_time_hr = []

        for article in news_articles:
            title = article.find("h3").find("a")["title"]
            link = article.find("h3").find("a")["href"]
            list_title_hr.append(title.strip())
            list_link_hr.append(link)
            list_source_hr.append("IT_HR")
            list_time_hr.append(date1)

    except Exception as e:
        print("ITnews", str(e))


    import requests
    from bs4 import BeautifulSoup

    url10 = "https://www.ndtv.com/topic/haryana"
    headers = {
        "User-Agent": "Your User Agent String"
    }

    # list_title_hr = []
    # list_link_hr = []
    # list_source_hr = []
    # list_time_hr = []

    try:
        r10 = requests.get(url10, headers=headers)

        soup10 = BeautifulSoup(r10.content, "html.parser")

        news_articles = soup10.find_all("li", class_="src_lst-li")

        for article in news_articles:
            title = article.find("div", class_="src_itm-ttl").find("a")["title"].replace("&quot;", "")
            link = article.find("div", class_="src_itm-ttl").find("a")["href"]
            list_title_hr.append(title.strip())
            list_link_hr.append(link)
            list_source_hr.append("NDTV_HR")
            list_time_hr.append(date1)

    except Exception as e:
        print("NDTVnews", str(e))


    import requests
    from bs4 import BeautifulSoup

    url10 = "https://www.business-standard.com/topic/haryana"
    headers = {
        "User-Agent": "Your User Agent String"
    }

    try:
        r10 = requests.get(url10, headers=headers)

        soup10 = BeautifulSoup(r10.content, "html.parser")

        news_articles = soup10.find_all("div", class_="cardlist")

        for article in news_articles:
            title_element = article.find("a", class_="smallcard-title")
            link_element = article.find("a", class_="smallcard-title")

            if title_element and link_element:
                title = title_element.text.strip()
                link = link_element["href"]

                list_title_hr.append(title)
                list_link_hr.append(link)
                list_source_hr.append("BS_HR")
                list_time_hr.append(date1)

    except Exception as e:
        print("BSnews", str(e))


    print(len(list_time_hr))
    print(len(list_source_hr))
    print(len(list_link_hr))
    print(len(list_title_hr))

    df_hr = pd.DataFrame(list(zip(list_source_hr,list_title_hr,list_link_hr,list_time_hr)),columns = ['source','titles','link','Time'])



    df_hr=df_hr.dropna()
    df_hr=df_hr[df_hr['titles']!='']
    df_hr.drop_duplicates(subset=['link'],keep=False,inplace=True)
    connection_url = 'postgresql://mindshare:mindshare@postgresql-76953-0.cloudclusters.net:/postgres'
    engine = create_engine(connection_url)
    connection = engine.connect()

    dump_hrdf = pd.read_sql_table('hrnews_dump_table', connection)


    compare_list=dump_hrdf['link'].unique().tolist()
#     hr_df=df_hr.copy()
    hr_df = df_hr[~df_hr["link"].isin(compare_list)]


    df_hr.shape

    hr_df.shape

    translator = Translator()
    # hr_df=df_hr.copy()
    # Function to translate text to English
    def translate_to_english(text):
        try:
            translation = translator.translate(text, src='auto', dest='en')
            return translation.text
        except Exception as e:
            print(e)
            return text

    # Apply translation function to the DataFrame and create a new column
    hr_df['titles'] = hr_df['titles'].apply(translate_to_english)

    hr_df=hr_df.dropna()
    hr_df.drop_duplicates(keep=False,inplace=True)
    hr_df=hr_df[hr_df['titles']!='']    
    ###################################################################################
    connection_url = 'postgresql://mindshare:mindshare@postgresql-76953-0.cloudclusters.net:19477/postgres'
    engine = create_engine(connection_url)
    connection = engine.connect()
    hr_df.to_sql('hrnews_dump_table',con=connection, if_exists='append',index=False)

    hr_df.to_excel('Haryana_Latest_News_'+date1+'.xlsx')


    # path='C:\Users\minds\OneDrive\Desktop\work amar DND\codes\scrapping\snscrape\Untitled Folder 3\TS voter list'

    # hr_df.to_excel('haryana_df_aaj.xlsx')
    hr_df['source'].value_counts()

    # date1=datetime.now()
    # date1 = date1.strftime("%d-%m-%Y %H-%M-%S")
    print(hr_df.shape)

    hr_df.to_excel('Haryana_Latest_News_'+date1+'.xlsx')
# CSV Formatting
    date1=datetime.now()
    date1 = date1.strftime("%d-%m-%Y %H-%M-%S")
    workbook = xlsxwriter.Workbook('Haryana_'+date1+'_mediatrackerexcel.xlsx')
    workbook.use_zip64()


    full_border3  = workbook.add_format({'border_color':"#000000",'align':'left','font_color':'blue','font_size':20})
    df=hr_df.copy()
    worksheet = workbook.add_worksheet()
    title = 'Media Tracker Report'
    worksheet.write('A1',title,full_border3)
    date = date.today()
    df = df[df['link'].notna()]
    df = df[df['titles'].notna()]

    k =1
    row =4

    header_format = workbook.add_format({'bold': True,'text_wrap': True,'align':'center','valign': 'top','fg_color': '#85C9E8','border': 1})
    col = ['Article','Link to Article']
    s =0

    worksheet.set_column(0, 0, 62)
    worksheet.set_column(1, 1, 24)


    for value in col:
        worksheet.write(3, s , value, header_format)
        s = s+1
    full_border1  = workbook.add_format({'border':1,'border_color':"#000000",'align':'left'})
    full_border2  = workbook.add_format({'border':1,'border_color':"#000000",'text_wrap':True})

    for item,value in df.iterrows():
        worksheet.write(row,0,value['titles'],full_border1)
        worksheet.write_url(row ,1,value['link'],full_border2,string = value['source'])

        row = row+1
    workbook.close()
#Autometed Email Mesage
    app = client.DispatchEx("Excel.Application")


    sheets = app.Workbooks.Open(r'C:\Users\minds\OneDrive\Desktop\work nitish DND\codes\scrapping\snscrape\Untitled Folder 3\TS voter list\Haryana_'+date1+'_mediatrackerexcel.xlsx')
    sheets.ExportAsFixedFormat(0, r'C:\Users\minds\OneDrive\Desktop\work nitish DND\codes\scrapping\snscrape\Untitled Folder 3\TS voter list\Haryana_'+date1+'_mediatrackerexcel.pdf')

    email_from = 'nitish15060@gmail.com'
    password = 'Xyaxysx'
    email_to = ['shikhar.amar@themindshare.in','nitish15060@gmail.com','sudhir.Chaudhary@themindshare.in','bijender.Sheoran@themindshare.in ']

    date_str = pd.Timestamp.today().strftime('%Y-%m-%d')

    email_message = MIMEMultipart()
    email_message['From'] = email_from
    email_message['To'] = ", ".join(email_to)
    email_message['Subject'] = f'Haryana Media Tracker Report Email - {date1}'
    text = ''

    part = MIMEBase('application', "octet-stream")
    part.set_payload(open(r'C:\Users\minds\OneDrive\Desktop\work amar DND\codes\scrapping\snscrape\Untitled Folder 3\TS voter list\Haryana_'+date1+'_mediatrackerexcel.pdf', "rb").read())
    encoders.encode_base64(part)
    part.add_header('Content-Disposition', 'attachment; filename="'r'Haryana_'+date1+'_mediatrackerexcel.pdf"')
    email_message.attach(part)


    email_message.attach(MIMEText(text))
    # Convert it as a string to send message
    email_string = email_message.as_string()

    context = ssl.create_default_context()
    with smtplib.SMTP_SSL("smtp.gmail.com", 465, context=context) as server:
        server.login(email_from, password)
        server.sendmail(email_from, email_to, email_string)
